{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en = spacy.load('en')\n",
    "de = spacy.load('de')\n",
    "\n",
    "file_name = './dataset/de-en/train.tags.de-en.en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3489132"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 19716472 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-ac1a76d734d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     temp = text[500:1000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             raise ValueError(Errors.E088.format(length=len(text),\n\u001b[0;32m--> 345\u001b[0;31m                                                 max_length=self.max_length))\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 19716472 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "with open(file_name) as f:\n",
    "    text = f.read()\n",
    "#     temp = text[500:1000]\n",
    "    doc = en(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unicode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2265fa220d62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     temp = text[500:1000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     for doc in en.pipe(text[500:],n_threads=-1,batch_size=1000):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unicode' is not defined"
     ]
    }
   ],
   "source": [
    "with open(file_name) as f:\n",
    "    text = f.read()\n",
    "    print(type(unicode(text)))\n",
    "#     temp = text[500:1000]\n",
    "#     for doc in en.pipe(text[500:],n_threads=-1,batch_size=1000):\n",
    "#         print(text[500:1500])\n",
    "#         print(\"doc\",doc)\n",
    "#         print(list(doc))\n",
    "#         break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ ,\n",
       " ridges,\n",
       " of,\n",
       " the,\n",
       " oceans,\n",
       " ',\n",
       " depths,\n",
       " ,,\n",
       " where,\n",
       " life,\n",
       " is,\n",
       " bizarre,\n",
       " ,,\n",
       " resilient,\n",
       " and,\n",
       " shockingly,\n",
       " abundant.</description,\n",
       " >,\n",
       " ,\n",
       " David,\n",
       " Gallo,\n",
       " :,\n",
       " This,\n",
       " is,\n",
       " Bill,\n",
       " Lange,\n",
       " .,\n",
       " I,\n",
       " 'm,\n",
       " Dave,\n",
       " Gallo,\n",
       " .,\n",
       " ,\n",
       " And,\n",
       " we,\n",
       " 're,\n",
       " going,\n",
       " to,\n",
       " tell,\n",
       " you,\n",
       " some,\n",
       " stories,\n",
       " from,\n",
       " the,\n",
       " sea,\n",
       " here,\n",
       " in,\n",
       " video,\n",
       " .,\n",
       " ,\n",
       " We,\n",
       " 've,\n",
       " got,\n",
       " some,\n",
       " of,\n",
       " the,\n",
       " most,\n",
       " incredible,\n",
       " video,\n",
       " of,\n",
       " Titanic,\n",
       " that,\n",
       " 's,\n",
       " ever,\n",
       " been,\n",
       " seen,\n",
       " ,,\n",
       " and,\n",
       " we,\n",
       " 're,\n",
       " not,\n",
       " going,\n",
       " to,\n",
       " show,\n",
       " you,\n",
       " any,\n",
       " of,\n",
       " it,\n",
       " .,\n",
       " ,\n",
       " The,\n",
       " truth,\n",
       " of,\n",
       " the,\n",
       " matter,\n",
       " is,\n",
       " that,\n",
       " the,\n",
       " Titanic,\n",
       " --,\n",
       " even,\n",
       " though,\n",
       " it,\n",
       " 's,\n",
       " breaking,\n",
       " all,\n",
       " sorts,\n",
       " of,\n",
       " box,\n",
       " office,\n",
       " records,\n",
       " --,\n",
       " it,\n",
       " 's,\n",
       " not,\n",
       " the,\n",
       " most,\n",
       " exciting,\n",
       " story,\n",
       " from,\n",
       " the,\n",
       " sea,\n",
       " .,\n",
       " ,\n",
       " And]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
